{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e9682d-78df-49d1-83d3-de492071a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from torchtext import datasets\n",
    "#from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a37ac8-3560-4efe-a229-d32fa2bd43ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device(type='cuda')\n",
    "else:\n",
    "    device=torch.device(type='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12624f1d-b989-4759-beb6-4ddaef7f94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe'>\n"
     ]
    }
   ],
   "source": [
    "train_data=datasets.IMDB(split='train')\n",
    "eval_data=datasets.IMDB(split='test')\n",
    "\n",
    "print(type(train_data))#type of train_data and test_data is iterdatapipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bac0d6-c5b4-4a79-84cc-33b1883a4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for preparing input-ground truth pair we only want the review \n",
    "train_map_data=[]\n",
    "\n",
    "for label,review in train_data:\n",
    "    train_map_data.append(review)\n",
    "\n",
    "\n",
    "eval_map_data=[]\n",
    "\n",
    "for label,review in train_data:\n",
    "    eval_map_data.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbe1d77-64f8-44e5-adcb-2ef95517c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(train_map_data))\n",
    "print(len(eval_map_data))\n",
    "\n",
    "#we will have 25000 reviews for training data and 25000 reviews for testing data \n",
    "#train_map_data and eval_map_data will have list of reviews and each review will have string datatype\n",
    "print(type(train_map_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8580b73f-da4b-43a0-8e1f-e99138315d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=get_tokenizer('basic_english',language='en')\n",
    "#this tokenizer will convert each review in list of  token words so it will help in build vocab and we can further modify how we want to tokenize out review for example we are spliting tokens by using whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85952705-e0fe-4aa5-8efa-191f2e7b8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vocab\n",
    "\n",
    "def build_vocab(train_map_data,tokenizer):\n",
    "    vocab=build_vocab_from_iterator(\n",
    "        map(tokenizer,train_map_data),\n",
    "        min_freq=3,\n",
    "        specials=[\"<unk>\"]\n",
    "    )\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "#this function will help in build vocab we are using build_vocab_from_iterator which we are taking from torchtext.vocab it have some parameters like map,special symbols,min_freq\n",
    "#map will have 2 arguments first will function which will applied on each token and that token will comes from second argument which is iterable list\n",
    "#specials means if some tokens is not part of our vocab that will consider as <unk> and if token comes at least  min_freq times in vocab then we will consider\n",
    "vocab=build_vocab(train_map_data,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6184d8e0-d3ca-4192-8fe7-e9d6e1f242f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40251\n"
     ]
    }
   ],
   "source": [
    "vocab_size=vocab.__len__()\n",
    "print(vocab_size) #vocab will have vocab data type actual token will start from 1 because 0 is unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3583546-fd48-464a-9bd4-a02806442f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=5\n",
    "max_norm=1\n",
    "max_seq_len=300\n",
    "embded_dim=100\n",
    "batch_size=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ae43a0-17e1-4b53-ad48-68271452a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline=lambda x:vocab(tokenizer(x)) #it convert list of tokens with list of numeric represention of token in that vocab means position of token in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f82fb7-21e9-4270-ae6d-cac222142af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_cbow(batch, text_pipeline):\n",
    "    \n",
    "     batch_input_words, batch_target_word = [], []\n",
    "     \n",
    "     for review in batch:\n",
    "        \n",
    "         review_tokens_ids = text_pipeline(review)\n",
    "            \n",
    "         if len(review_tokens_ids) < window_size * 2 + 1:\n",
    "             continue\n",
    "                \n",
    "         if max_seq_len:\n",
    "             review_tokens_ids = review_tokens_ids[:max_seq_len]\n",
    "             \n",
    "         for idx in range(len(review_tokens_ids) - window_size * 2):\n",
    "             current_ids_sequence = review_tokens_ids[idx : (idx + window_size * 2 + 1)]\n",
    "             target_word = current_ids_sequence.pop(window_size)\n",
    "             input_words = current_ids_sequence\n",
    "             batch_input_words.append(input_words)\n",
    "             batch_target_word.append(target_word)\n",
    "     \n",
    "     batch_input_words = torch.tensor(batch_input_words, dtype=torch.long)\n",
    "     batch_target_word = torch.tensor(batch_target_word, dtype=torch.long)\n",
    "     \n",
    "     return batch_input_words, batch_target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb8acb1d-8e08-41bf-ac72-fefa3d25e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_skipgram(batch, text_pipeline):\n",
    "    \n",
    "    batch_input_word, batch_target_words = [], []\n",
    "    \n",
    "    for review in batch:\n",
    "        review_tokens_ids = text_pipeline(review)\n",
    "\n",
    "        if len(review_tokens_ids) < window_size * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if max_seq_len:\n",
    "            review_tokens_ids = review_tokens_ids[:max_seq_len]\n",
    "\n",
    "        for idx in range(len(review_tokens_ids) - window_size * 2):\n",
    "            current_ids_sequence = review_tokens_ids[idx : (idx + window_size * 2 + 1)]\n",
    "            input_word = current_ids_sequence.pop(window_size)\n",
    "            target_words = current_ids_sequence\n",
    "\n",
    "            for target_word in target_words:\n",
    "                batch_input_word.append(input_word)\n",
    "                batch_target_words.append(target_word)\n",
    "\n",
    "    batch_input_word = torch.tensor(batch_input_word, dtype=torch.long)\n",
    "    batch_target_words = torch.tensor(batch_target_words, dtype=torch.long)\n",
    "    return batch_input_word, batch_target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e79db19-b33c-4791-836c-374dd915de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl_cbow = DataLoader(\n",
    "        train_map_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_cbow,text_pipeline=text_pipeline)\n",
    "    )\n",
    "\n",
    "traindl_skipgram = DataLoader(\n",
    "        train_map_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_skipgram,text_pipeline=text_pipeline)\n",
    "    )\n",
    "\n",
    "evaldl_cbow = DataLoader(\n",
    "        eval_map_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_cbow,text_pipeline=text_pipeline)\n",
    "    )\n",
    "\n",
    "evaldl_skipgram = DataLoader(\n",
    "        eval_map_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_skipgram,text_pipeline=text_pipeline)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05db03a1-3cc5-49db-8204-b0eb5aa7869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embded_dim,\n",
    "            max_norm=max_norm\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embded_dim,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Shape of x before embedding:\",x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        #print(\"Shape of x after embedding:\",x.shape)\n",
    "        x = x.mean(axis=1)\n",
    "        #print(\"Shape of x after mean:\",x.shape)\n",
    "        x = self.linear(x)\n",
    "        #print(\"Shape of x at the end of forward:\",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d41f73-b813-431e-8383-3f37f74fa1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            max_norm=max_norm\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Shape of x before embedding:\",x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        #print(\"Shape of x after embedding:\",x.shape)\n",
    "        x = self.linear(x)\n",
    "        #print(\"Shape of x at the end of forward:\",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "786f6357-ccfe-4df4-99cc-e46c618a5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model,dataloader,loss_fn,opt):\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "\n",
    "    for i, batch_data in enumerate(dataloader):\n",
    "        inputs = batch_data[0].to(device)\n",
    "        targets = batch_data[1].to(device)\n",
    "        #print(\"Input Shape:\",inputs.shape, \"Target Shape:\",targets.shape)\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        print('current loss in batch',loss.item())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "    epoch_loss = np.mean(running_loss)\n",
    "    print(\"Train Epoch Loss:\",round(epoch_loss,3))\n",
    "    loss_dict[\"train\"].append(epoch_loss)\n",
    "\n",
    "def validate_one_epoch(model,dataloader,loss_fn):\n",
    "    model.eval()\n",
    "    running_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(dataloader, 1):\n",
    "            inputs = batch_data[0].to(device)\n",
    "            targets = batch_data[1].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            print(\"current loss in batch\",loss.item())\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "\n",
    "    epoch_loss = np.mean(running_loss)\n",
    "    print(\"Validation Epoch Loss:\",round(epoch_loss,3))\n",
    "    loss_dict[\"val\"].append(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd602a96-b95d-4f12-bc27-9ce9c4782264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW model selected.\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "n_epochs = 5\n",
    "loss_dict = {\"train\": [], \"val\": []}\n",
    "\n",
    "# Loop until valid input is provided\n",
    "while True:\n",
    "    choice = input(\"Enter 'cbow' or 'skipgram': \").lower()  # Convert to lowercase for case-insensitivity\n",
    "    if choice == \"cbow\":\n",
    "        model = CBOW(vocab_size).to(device)\n",
    "        dataloader_train = traindl_cbow\n",
    "        dataloader_val = evaldl_cbow\n",
    "        print(\"CBOW model selected.\")\n",
    "        break\n",
    "    elif choice == \"skipgram\":\n",
    "        model = SkipGram(vocab_size).to(device)\n",
    "        dataloader_train = traindl_skipgram\n",
    "        dataloader_val = evaldl_skipgram\n",
    "        print(\"SkipGram model selected.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice! Please enter 'cbow' or 'skipgram'.\")\n",
    "\n",
    "\n",
    "opt = optim.Adam(params=model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a697b0-4f49-499a-ae13-0f52f5320288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 1\n",
      "current loss in batch 10.607257843017578\n",
      "current loss in batch 10.602150917053223\n",
      "current loss in batch 10.600223541259766\n",
      "current loss in batch 10.596548080444336\n",
      "current loss in batch 10.59465503692627\n",
      "current loss in batch 10.58989143371582\n",
      "current loss in batch 10.586722373962402\n",
      "current loss in batch 10.58370590209961\n",
      "current loss in batch 10.579516410827637\n",
      "current loss in batch 10.57573127746582\n",
      "current loss in batch 10.57209587097168\n",
      "current loss in batch 10.568284034729004\n",
      "current loss in batch 10.56435775756836\n",
      "current loss in batch 10.560440063476562\n",
      "current loss in batch 10.555243492126465\n",
      "current loss in batch 10.549639701843262\n",
      "current loss in batch 10.54469108581543\n",
      "current loss in batch 10.540741920471191\n",
      "current loss in batch 10.535109519958496\n",
      "current loss in batch 10.53193473815918\n",
      "current loss in batch 10.523679733276367\n",
      "current loss in batch 10.517114639282227\n",
      "current loss in batch 10.506868362426758\n",
      "current loss in batch 10.502496719360352\n",
      "current loss in batch 10.490584373474121\n",
      "current loss in batch 10.488168716430664\n",
      "current loss in batch 10.48373794555664\n",
      "current loss in batch 10.472980499267578\n",
      "current loss in batch 10.46683406829834\n",
      "current loss in batch 10.458598136901855\n"
     ]
    }
   ],
   "source": [
    "for e in range(n_epochs):\n",
    "    print(\"Epoch=\",e+1)\n",
    "    train_one_epoch(model,dataloader_train,loss_fn,opt)\n",
    "    validate_one_epoch(model,dataloader_val,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fff57a-bb61-4395-b318-b5d2c314d938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
