{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e9682d-78df-49d1-83d3-de492071a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from torchtext import datasets\n",
    "#from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a37ac8-3560-4efe-a229-d32fa2bd43ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device(type='cuda')\n",
    "else:\n",
    "    device=torch.device(type='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12624f1d-b989-4759-beb6-4ddaef7f94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe'>\n"
     ]
    }
   ],
   "source": [
    "train_data=datasets.IMDB(split='train')\n",
    "eval_data=datasets.IMDB(split='test')\n",
    "\n",
    "print(type(train_data))#type of train_data and test_data is iterdatapipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bac0d6-c5b4-4a79-84cc-33b1883a4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for preparing input-ground truth pair we only want the review \n",
    "train_map_data=[]\n",
    "\n",
    "for label,review in train_data:\n",
    "    train_map_data.append(review)\n",
    "\n",
    "\n",
    "eval_map_data=[]\n",
    "\n",
    "for label,review in train_data:\n",
    "    eval_map_data.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbe1d77-64f8-44e5-adcb-2ef95517c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(train_map_data))\n",
    "print(len(eval_map_data))\n",
    "\n",
    "#we will have 25000 reviews for training data and 25000 reviews for testing data \n",
    "#train_map_data and eval_map_data will have list of reviews and each review will have string datatype\n",
    "print(type(train_map_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8580b73f-da4b-43a0-8e1f-e99138315d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=get_tokenizer('basic_english',language='en')\n",
    "#this tokenizer will convert each review in list of  token words so it will help in build vocab and we can further modify how we want to tokenize out review for example we are spliting tokens by using whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85952705-e0fe-4aa5-8efa-191f2e7b8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vocab\n",
    "\n",
    "def build_vocab(train_map_data,tokenizer):\n",
    "    vocab=build_vocab_from_iterator(\n",
    "        map(tokenizer,train_map_data),\n",
    "        min_freq=3,\n",
    "        specials=[\"<unk>\"]\n",
    "    )\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "#this function will help in build vocab we are using build_vocab_from_iterator which we are taking from torchtext.vocab it have some parameters like map,special symbols,min_freq\n",
    "#map will have 2 arguments first will function which will applied on each token and that token will comes from second argument which is iterable list\n",
    "#specials means if some tokens is not part of our vocab that will consider as <unk> and if token comes at least  min_freq times in vocab then we will consider\n",
    "vocab=build_vocab(train_map_data,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6184d8e0-d3ca-4192-8fe7-e9d6e1f242f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40251\n"
     ]
    }
   ],
   "source": [
    "vocab_size=vocab.__len__()\n",
    "print(vocab_size) #vocab will have vocab data type actual token will start from 1 because 0 is unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3583546-fd48-464a-9bd4-a02806442f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=5\n",
    "max_norm=1\n",
    "max_seq_len=300\n",
    "embded_dim=100\n",
    "batch_size=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ae43a0-17e1-4b53-ad48-68271452a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline=lambda x:vocab(tokenizer(x)) #it convert list of tokens with list of numeric represention of token in that vocab means position of token in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f82fb7-21e9-4270-ae6d-cac222142af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_cbow(batch, text_pipeline):\n",
    "    \n",
    "     batch_input_words, batch_target_word = [], []\n",
    "     \n",
    "     for review in batch:\n",
    "        \n",
    "         review_tokens_ids = text_pipeline(review)\n",
    "            \n",
    "         if len(review_tokens_ids) < window_size * 2 + 1:\n",
    "             continue\n",
    "                \n",
    "         if max_seq_len:\n",
    "             review_tokens_ids = review_tokens_ids[:max_seq_len]\n",
    "             \n",
    "         for idx in range(len(review_tokens_ids) - window_size * 2):\n",
    "             current_ids_sequence = review_tokens_ids[idx : (idx + window_size * 2 + 1)]\n",
    "             target_word = current_ids_sequence.pop(window_size)\n",
    "             input_words = current_ids_sequence\n",
    "             batch_input_words.append(input_words)\n",
    "             batch_target_word.append(target_word)\n",
    "     \n",
    "     batch_input_words = torch.tensor(batch_input_words, dtype=torch.long)\n",
    "     batch_target_word = torch.tensor(batch_target_word, dtype=torch.long)\n",
    "     \n",
    "     return batch_input_words, batch_target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb8acb1d-8e08-41bf-ac72-fefa3d25e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_skipgram(batch, text_pipeline):\n",
    "    \n",
    "    batch_input_word, batch_target_words = [], []\n",
    "    \n",
    "    for review in batch:\n",
    "        review_tokens_ids = text_pipeline(review)\n",
    "\n",
    "        if len(review_tokens_ids) < window_size * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if max_seq_len:\n",
    "            review_tokens_ids = review_tokens_ids[:max_seq_len]\n",
    "\n",
    "        for idx in range(len(review_tokens_ids) - window_size * 2):\n",
    "            current_ids_sequence = review_tokens_ids[idx : (idx + window_size * 2 + 1)]\n",
    "            input_word = current_ids_sequence.pop(window_size)\n",
    "            target_words = current_ids_sequence\n",
    "\n",
    "            for target_word in target_words:\n",
    "                batch_input_word.append(input_word)\n",
    "                batch_target_words.append(target_word)\n",
    "\n",
    "    batch_input_word = torch.tensor(batch_input_word, dtype=torch.long)\n",
    "    batch_target_words = torch.tensor(batch_target_words, dtype=torch.long)\n",
    "    return batch_input_word, batch_target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e79db19-b33c-4791-836c-374dd915de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl_cbow = DataLoader(\n",
    "        train_map_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_cbow,text_pipeline=text_pipeline)\n",
    "    )\n",
    "\n",
    "traindl_skipgram = DataLoader(\n",
    "        train_map_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_skipgram,text_pipeline=text_pipeline)\n",
    "    )\n",
    "\n",
    "evaldl_cbow = DataLoader(\n",
    "        eval_map_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_cbow,text_pipeline=text_pipeline)\n",
    "    )\n",
    "\n",
    "evaldl_skipgram = DataLoader(\n",
    "        eval_map_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_skipgram,text_pipeline=text_pipeline)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05db03a1-3cc5-49db-8204-b0eb5aa7869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embded_dim,\n",
    "            max_norm=max_norm\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embded_dim,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Shape of x before embedding:\",x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        #print(\"Shape of x after embedding:\",x.shape)\n",
    "        x = x.mean(axis=1)\n",
    "        #print(\"Shape of x after mean:\",x.shape)\n",
    "        x = self.linear(x)\n",
    "        #print(\"Shape of x at the end of forward:\",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d41f73-b813-431e-8383-3f37f74fa1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            max_norm=max_norm\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Shape of x before embedding:\",x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        #print(\"Shape of x after embedding:\",x.shape)\n",
    "        x = self.linear(x)\n",
    "        #print(\"Shape of x at the end of forward:\",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "786f6357-ccfe-4df4-99cc-e46c618a5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model,dataloader):\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "\n",
    "    for i, batch_data in enumerate(dataloader):\n",
    "        inputs = batch_data[0].to(device)\n",
    "        targets = batch_data[1].to(device)\n",
    "        #print(\"Input Shape:\",inputs.shape, \"Target Shape:\",targets.shape)\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "    epoch_loss = np.mean(running_loss)\n",
    "    print(\"Train Epoch Loss:\",round(epoch_loss,3))\n",
    "    loss_dict[\"train\"].append(epoch_loss)\n",
    "\n",
    "def validate_one_epoch(model,dataloader):\n",
    "    model.eval()\n",
    "    running_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(dataloader, 1):\n",
    "            inputs = batch_data[0].to(device)\n",
    "            targets = batch_data[1].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "\n",
    "    epoch_loss = np.mean(running_loss)\n",
    "    print(\"Validation Epoch Loss:\",round(epoch_loss,3))\n",
    "    loss_dict[\"val\"].append(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd602a96-b95d-4f12-bc27-9ce9c4782264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW model selected.\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "n_epochs = 5\n",
    "loss_dict = {\"train\": [], \"val\": []}\n",
    "\n",
    "# Loop until valid input is provided\n",
    "while True:\n",
    "    choice = input(\"Enter 'cbow' or 'skipgram': \").lower()  # Convert to lowercase for case-insensitivity\n",
    "    if choice == \"cbow\":\n",
    "        model = CBOW(vocab_size).to(device)\n",
    "        dataloader_train = traindl_cbow\n",
    "        dataloader_val = evaldl_cbow\n",
    "        print(\"CBOW model selected.\")\n",
    "        break\n",
    "    elif choice == \"skipgram\":\n",
    "        model = SkipGram(vocab_size).to(device)\n",
    "        dataloader_train = traindl_skipgram\n",
    "        dataloader_val = evaldl_skipgram\n",
    "        print(\"SkipGram model selected.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice! Please enter 'cbow' or 'skipgram'.\")\n",
    "\n",
    "\n",
    "opt = optim.Adam(params=model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92a697b0-4f49-499a-ae13-0f52f5320288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch=\u001b[39m\u001b[38;5;124m\"\u001b[39m,e\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     validate_one_epoch(model,dataloader_val)\n",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m running_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(n_epochs):\n",
    "    print(\"Epoch=\",e+1)\n",
    "    train_one_epoch(model,dataloader_train)\n",
    "    validate_one_epoch(model,dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fff57a-bb61-4395-b318-b5d2c314d938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
